{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Melbourne Housing Prices Prediction\n",
    "\n",
    "This notebook shows initial cleaning and feature engineering on the Melbourne Housing dataset.\n",
    "\n",
    "The data is from Kaggle and can be found [here](https://www.kaggle.com/anthonypino/melbourne-housing-market)\n",
    "\n",
    "Prices in the set are predicted using a Random Forest regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import pandas_profiling as pdpr\n",
    "\n",
    "import matplotlib.pyplot as pyplot\n",
    "pyplot.rcParams['figure.dpi'] = 300\n",
    "pyplot.rcParams['savefig.dpi'] = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "First, let's laod the data and split of a test set to gage our performance\n",
    "gainst at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pd.read_csv(\"./data/Melbourne_housing_FULL.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = full_data.loc[:, 'Price']\n",
    "X = full_data.drop(columns=['Price'])\n",
    "\n",
    "train_size = 0.8\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=train_size, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning\n",
    "\n",
    "We cannot predict anything where the target is missing. Add the target value\n",
    "back to the dataset and drop all rows that have a missing target. Then separate\n",
    "the features and target again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full = pd.concat([X_train, y_train], axis=1)\n",
    "train_with_target = train_full.dropna(subset=['Price'])\n",
    "\n",
    "X_train = train_with_target.drop(columns=['Price'])\n",
    "y_train = train_with_target.loc[:, 'Price']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small helper to ease creating dummies for various variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dummies(df: pd.DataFrame, column: str) -> None:\n",
    "    \"\"\"Turns the indicated column into dummies, omitting the first. Removes\n",
    "    the original column.\n",
    "\n",
    "    Args:\n",
    "        frame (pd.DataFrame): Input frame containing column.\n",
    "        column (str): Name of the column to turn into dummies.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    dummies = pd.get_dummies(\n",
    "        df.loc[:, column], prefix=column, drop_first=True)\n",
    "    df = pd.concat([X_train, dummies], axis=1)\n",
    "    df.drop(columns=[column], inplace=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adress has too many unique values, we drop it. We also drop Method and\n",
    "SellerG as these showed no effect during cross-validation.\n",
    "\n",
    "We also drop Bedrooms2 since it is highly correlated with Rooms and shows no\n",
    "effect in cross-validation.\n",
    "\n",
    "Buiding Area has too many missing values, so gets dropped too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.loc[:, 'Address'].value_counts())\n",
    "X_train.drop(columns=['Address'], inplace=True)\n",
    "\n",
    "X_train.drop(columns=['Method'], inplace=True)\n",
    "X_train.drop(columns=['SellerG'], inplace=True)\n",
    "\n",
    "X_train.drop(columns=['Bedroom2'], inplace=True)\n",
    "\n",
    "print(\"BuildingArea Missing\", X_train.loc[:,'BuildingArea'].isnull().sum())\n",
    "X_train.drop(columns=['BuildingArea'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type is categorical but not ordinal so we convert it to dummies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.loc[:, 'Type'].value_counts())\n",
    "X_train = make_dummies(X_train, 'Type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep only the year sold from date, as Month showed no effect in cross-\n",
    "validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.loc[:, 'Date'] = pd.to_datetime(X_train.loc[:, 'Date'])\n",
    "X_train.loc[:, 'Year_sold'] = X_train.loc[:, 'Date'].dt.year\n",
    "X_train.drop(columns=['Date'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distance only has one missing value, which we fill with the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance -keep and fill one missing\n",
    "print(\"Missing in Distance:\", X_train.loc[:,'Distance'].isnull().sum())\n",
    "median_dist = X_train.loc[:, 'Distance'].dropna().mean()\n",
    "X_train.loc[:, 'Distance'].fillna(median_dist, inplace=True)\n",
    "X_train.loc[:, 'Distance'] = np.log(X_train.loc[:, 'Distance']+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bathroom is also correlated with Rooms (i.e. property size) but we can keep it\n",
    "as Bathrooms_per_room which shows some significance in cross-validation after\n",
    "filling missing values with the mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Bathroom missing\", X_train.loc[:, 'Bathroom'].isnull().sum())\n",
    "mode_bathroom = X_train.loc[:, 'Bathroom'].mode()[0]\n",
    "X_train.loc[:, 'Bathroom'].fillna(mode_bathroom, inplace=True)\n",
    "\n",
    "X_train.loc[:, 'Bathroom_per_room'] =\\\n",
    "    X_train.loc[:, 'Bathroom'] / X_train.loc[:, 'Rooms']\n",
    "\n",
    "X_train.drop(columns=['Bathroom'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same logic applies to Car."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Car missing\", X_train.loc[:, 'Car'].isnull().sum())\n",
    "\n",
    "car_mode = X_train.loc[:, 'Car'].mode()[0]\n",
    "X_train.loc[:, 'Car'].fillna(car_mode, inplace=True)\n",
    "\n",
    "X_train.loc[:, 'Car_per_room'] =\\\n",
    "    X_train.loc[:, 'Car'] / X_train.loc[:, 'Rooms']\n",
    "X_train.drop(columns=['Car'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Year built is mostly missing but where present it has a strong effect. To \n",
    "capture this effect we turn it into categories, with one for missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.loc[:, 'YearBuilt'] = pd.cut(\n",
    "    X_train.loc[:, 'YearBuilt'],\n",
    "    bins=[0, 1800, 1900, 1945, 2000, 3000],\n",
    "    labels=['1800s', '1900s', 'prewar', 'postwar', 'new'],\n",
    "    ordered=False\n",
    ").astype(str)\n",
    "\n",
    "X_train.loc[:, 'YearBuilt'].fillna('unknown', inplace=True)\n",
    "\n",
    "X_train = make_dummies(X_train, 'YearBuilt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Location is crucial in real estate, so we do our best to fill missing \n",
    "location values from other available data, falling back to just the council\n",
    "mean if closer mean cannot be calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_locs = X_train\\\n",
    "    .loc[:, ['CouncilArea', 'Postcode', 'Suburb', 'Lattitude', 'Longtitude']]\\\n",
    "    .groupby(['CouncilArea', 'Postcode', 'Suburb'])\\\n",
    "    .mean()\n",
    "\n",
    "mean_locs_council = X_train\\\n",
    "    .loc[:, ['CouncilArea', 'Lattitude', 'Longtitude']]\\\n",
    "    .groupby(['CouncilArea'])\\\n",
    "    .mean()\n",
    "\n",
    "msk_empty_locs =\\\n",
    "    (X_train.loc[:, 'Lattitude'].isnull()) |\\\n",
    "    (X_train.loc[:, 'Lattitude'].isnull())\n",
    "\n",
    "for index, item in X_train.loc[msk_empty_locs, :].iterrows():\n",
    "    # try to fill the mean matching that location\n",
    "    try:\n",
    "        X_train.loc[index, 'Lattitude'] = mean_locs\\\n",
    "            .loc[item['CouncilArea']]\\\n",
    "            .loc[item['Postcode']]\\\n",
    "            .loc[item['Suburb'], \"Lattitude\"]\n",
    "        X_train.loc[index, 'Longtitude'] = mean_locs\\\n",
    "            .loc[item['CouncilArea']]\\\n",
    "            .loc[item['Postcode']]\\\n",
    "            .loc[item['Suburb'], 'Longtitude']\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "msk_empty_locs =\\\n",
    "    (X_train.loc[:, 'Lattitude'].isnull()) |\\\n",
    "    (X_train.loc[:, 'Lattitude'].isnull())\n",
    "\n",
    "for index, item in X_train.loc[msk_empty_locs, :].iterrows():\n",
    "    # try to fill based just on CouncilArea\n",
    "    try:\n",
    "        X_train.loc[index, 'Lattitude'] = mean_locs_council\\\n",
    "            .loc[item['CouncilArea'], 'Lattitude']\n",
    "        X_train.loc[index, 'Longtitude'] = mean_locs_council\\\n",
    "            .loc[item['CouncilArea'], 'Longtitude']\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "# Fill any remaining missing with the overall mean\n",
    "mean_lat = X_train.loc[:, 'Lattitude'].mean()\n",
    "mean_long = X_train.loc[:, 'Longtitude'].mean()\n",
    "\n",
    "X_train.loc[:, 'Lattitude'].fillna(mean_lat, inplace=True)\n",
    "X_train.loc[:, 'Longtitude'].fillna(mean_long, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After imputing lat/long we no longer need the geographical categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop(columns=['CouncilArea', 'Postcode', 'Suburb'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To capture interaction effects between lattitude, logitude, and distance we add\n",
    "a normalised product of these. This may caputure specific city location effects\n",
    "better than the individual terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_lat = X_train.loc[:, 'Lattitude'].mean()\n",
    "mean_long = X_train.loc[:, 'Longtitude'].mean()\n",
    "mean_dist = X_train.loc[:, 'Distance'].mean()\n",
    "\n",
    "X_train.loc[:, 'LatLong'] = (X_train.loc[:, 'Lattitude'] - mean_lat) * (\n",
    "    X_train.loc[:, \"Longtitude\"] - mean_long) * (X_train.loc[:, 'Distance'] - mean_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distance no longer contributes in cross-validation after this change, so we\n",
    "drop it later, but keep it for now to imput Landsize.\n",
    "\n",
    "Landsize has many missing values holds some potential information. We correct\n",
    "the outliers in the 99th percentile and fill missing values using nearest\n",
    "neighbours by location and type of property.\n",
    "\n",
    "To reduce the right skew we apply a log transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landsize_99p = np.percentile(X_train.loc[:, 'Landsize'].values, [99.0])[0]\n",
    "\n",
    "msk_over99perc_landsize = X_train.loc[:, 'Landsize'] > landsize_99p\n",
    "X_train.loc[msk_over99perc_landsize, 'Landsize'] = landsize_99p\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=100)\n",
    "\n",
    "imputer.fit(X_train.loc[:, ['Type_t', 'Type_u', 'Lattitude',\n",
    "                            'Longtitude', 'Distance', 'Landsize']]\n",
    "            )\n",
    "X_train.loc[:, 'Landsize'] = imputer\\\n",
    "    .transform(X_train.loc[:, ['Type_t', 'Type_u', 'Lattitude',\n",
    "                               'Longtitude', 'Distance', 'Landsize']])[:, 5]\n",
    "\n",
    "landsize_mean = X_train.loc[:, 'Landsize'].mean()\n",
    "X_train.loc[:, 'Landsize'] = np.log(X_train.loc[:, 'Landsize'] + landsize_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can drop distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop(columns=['Distance'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Region does not add any information beyond lat/long so we drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop(columns=['Regionname'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Propertycount only has a few missing values that we will with the median. We\n",
    "also transform it to reduce the skewness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Propertycount - keep this, already numerical - fill couple of missing with mean\n",
    "median_pcount = X_train.loc[:, 'Propertycount'].mean()\n",
    "X_train.loc[:, 'Propertycount'].fillna(median_pcount, inplace=True)\n",
    "X_train.loc[:, 'Propertycount'] = np.sqrt(X_train.loc[:, 'Propertycount'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the number of rooms is vital but again we transform it to reduce\n",
    "skeweness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.loc[:, 'Rooms'] = np.sqrt(X_train.loc[:, 'Rooms'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That leaves us with a clean set of colums to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory\n",
    "\n",
    "Three sets of pandas profiling output (all values, target cleaned, and after\n",
    "cleaning) were used in assessing the above cleaning steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = pdpr.ProfileReport(train_full)\n",
    "profile.to_file(output_file='profile.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = pdpr.ProfileReport(train_with_target)\n",
    "profile.to_file(output_file='profile_cleaned_target.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = pdpr.ProfileReport(X_train)\n",
    "profile.to_file(output_file='profile_clean_features.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "We fit a linear regression and a random forrest regressor to the cleaned data,\n",
    "applying a standard scalar first in each case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_rf = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', RandomForestRegressor(min_samples_split=10))\n",
    "])\n",
    "\n",
    "rf_cv = cross_validate(\n",
    "    pipe_rf,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    return_estimator=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RF scores:\", np.round(np.sqrt(-rf_cv['test_score'])))\n",
    "print(\"RF mean score:\", np.round(np.mean(np.sqrt(-rf_cv['test_score']))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean squared error is still quite high. As a next step features\n",
    "should be reviewed again for possible improvements before hyperparameters of\n",
    "the random forest are tuned before a final evaluation against the test set."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3ac8971da6bcbe4a1b80bd464d2d1540223112fa77eed9c024ebc2e75dde27d6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('melbourne-housing': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
